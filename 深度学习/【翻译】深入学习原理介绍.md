---
title: 深入学习原理介绍
tags: 新建,模板,小书匠
grammar_cjkRuby: true
---


# 深度学习原理介绍
下文为深度学习的最新概念:
[See the following article for a recent survey of deep learning:][1]

Yoshua Bengio, Learning Deep Architectures for AI, Foundations and Trends in Machine Learning, 2(1), 2009

## 深度
从一个输入产生一个输出的计算过程，可以表示为一个流程图: 该流程图表示一个计算，每个节点表示一个基本的运算和值(计算的结果，该值被该节点的子节点所接受)。考虑每个节点允许的计算集和可能的图结构，这定义了一个函数族。输入节点无子节点。输出节点无父节点。

表达式sin(a^2+b/a)的流程可以表示为: 
	2个数据节点a和b, 一个节点用于计算 b/a，并接受a和b作为输入。
	一个节点进行平方计算，仅接受a作为输入
	一个节点用于求和
	一个输出节点进行了sin计算

上述流程图中的一个重要属性是**深度**：是指流程图中从输入到输出的最长路径。

## 人工神经网络
深度学习是基于人工神经网络发展起来的一项技术。
人工神经网络（Artificial Neural Network，即ANN ），是20世纪80 年代以来人工智能领域兴起的研究热点。它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。在工程与学术界也常直接简称为神经网络或类神经网络。神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数（activation function）。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。

人工神经网络模型主要考虑网络连接的拓扑结构、神经元的特征、学习规则等。目前，已有近40种神经网络模型，其中有反传网络、感知器、自组织映射、Hopfield网络、波耳兹曼机、适应谐振理论等。根据连接的拓扑结构，神经网络模型可以分为：

**前向网络**
网络中各个神经元接受前一级的输入，并输出到下一级，网络中没有反馈，可以用一个有向无环路图表示。这种网络实现信号从输入空间到输出空间的变换，它的信息处理能力来自于简单非线性函数的多次复合。网络结构简单，易于实现。反传网络是一种典型的前向网络。
**反馈网络**
网络内神经元间有反馈，可以用一个无向的完备图表示。这种神经网络的信息处理是状态的变换，可以用动力学系统理论处理。系统的稳定性与联想记忆功能有密切关系。Hopfield网络、波耳兹曼机均属于这种类型。

## 深度学习
在探寻人工神经网络的过程中，人们逐渐认识到**深度**的重要性。主要体现在:

### 深度不足造成的问题
深度为2在很多情况下足以表达任给出何精确目标的过程(如，逻辑门， formal [threshold] neurons, sigmoid-neurons, Radial Basis Function [RBF] units like in SVMs)。但这样也会付出代价: 在图中所需的节点数会非常多。推理结果标识现存的过程复杂度和输入数量成正比( O(n) );当而所需节点数会激增到( O(2^n) ) .

> 换句人话就是：如果用深度为2的排列组合的方式组织推理，在解决复杂问题时，所需的计算量大得惊人。

### 认知的过程似乎具备深度特征
(TODO)

### 人脑具备深度特征
(TODO)


## 深度学习的现状
### 深度学习适合解决什么问题？
广义上说，深度学习是一种计算机无监督学习的方法。它在下面几个领域取得了明显的效果:

 - 图像识别 
 - 语音识别 
 - 自然语言理解

### 深度学习的著名成果有哪些?

**Google的深度学习**
最初该项目使用16000个CPU，1000万个YouyTube视频进行训练。算法自己学会了识别猫脸；
后来谷歌在该项目的基础上，基于8000个GPU构建了TensorFlow。
谷歌深度学习的应用包括:

 - 语音识别 
 - 图像识别/图像类别是被
 -  图像搜索 
 -  街景图像识别(包含文字) 
 -  自然语言理解 
 -  翻译

在这些应用中，背后都有TensorFlow平台的深度学习技术作为支撑

2016年5月，谷歌发布了专门用于深度学习的芯片TPU（TensorFlow Processing Unit）。

**Facebook取得的成果**
2016年Facebook开源了其深度学习框架Torchnet。
Torchnet使用Lua脚本语言开发，可以运行于X86CPU或GPU上。
Torchnet的主要意义在于通过模块化的设计，便于深度学习的实验变得容易、简单、快速，大大减少了从零开始设计负责的深度学习代码和模块所需的时间。Torchne在工程化方面，可说是意义重大。

Facebook自身还法波了人工智能产品DeepText，它能够准确识别人类聊天内容。该技术已用于以下方面:

 - 对Facebook的海量内容进行分类
 -  机器人介入聊天场景
 -   Facebook上的垃圾消息识别


**百度的深度学习**

 - 百度OCR：光学字符识别
 - 商品图像搜索
 - 在线广告
 - 以图搜图
 - 语音是被
 - 开源深度学习平台MXNet
 - Warp-CTC： 改进的语音识别系统

**阿里巴巴深度学习**

 - 拍立淘
 - 智能客服Messenger

**京东深度学习**

 2014年京东称了 了深度神经网络实验室(JD DNN Lab)。该实验室的主要成就是京东客服机器人(JDIMI)。JDIMI可用于售前咨询、售后服务等自动问答场景。JDIMI中涉及如下技术:
 

 - 自然语言处理
 - 用户画像
 - 知识图谱
 - 机器学习
 - 深度学习

**腾讯深度学习**
腾讯的深度学习平台Mariana，针对腾讯多种应用打造了三套框架:

 - Mariana DNN: 用于语音识别
 - Mariana CNN: 用于图像识别
 - Mariana Cluster: 用于广告点击率预估模型的训练

**其他科技创业公司成就**

 - Face++ : 支付宝人脸支付技术提供方
 - Linkface: 以基于深度学习的人脸识别为核心技术，产品用于: 京东钱包、融360、公安部第三研究所。

## 关于深度学习的现状总结

 - 构建深度学习网络需要大量硬件资源
 - 深度学习在软件安装和配置方面的门槛较高
 - 深度学习的最大问题是：需要海量的有标注的数据作为支撑
 - 深度学习的最后阶段竟然变成枯燥、机械、及其耗时的调参工作

